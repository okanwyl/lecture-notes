program testing : testing with artifitical data

can reveal the presence of errors not their absence

testing is a more general v&v which also includes static validation techniques

static techniques example-> handmade reviews generally early stage
dynamic testing -> funcitonal, non-functionaL-> unit testing integration testing system testing etc. 
later stage


program testing goals


diffrences of validation testing and defect testing

validaton testing purpose -> requirement analysis results should be tested
if the software is generic all over of the system should be tested

defect testing purpose -> which behavior is incorrect

goals of validation testing and defect testing

validation test -> a successful test shows that the system operates as intended

defect test -> a successful test makes the system perform incorrectly so that exposes a defect in the
system


verification and validation

verification -> are we building product right?
validation -> are we building right product?

aim of the v&v establish confidence that the system is 'fit for purpose'
diffrences of purposes
1. software purpose -> level of confidence depends on organization
2. user expectations -> users may have low expectations of certain kind of software
3. marketing enviorment -> market early maybe exposes defects.

#verification techniques -> peer reviews, walkthrough, inspections really?

#inspections 

inspections and testing are complementary and not opposing verification techniques
both should be used V&V
inspections cannot check non-functional characteristics such as  performance, usability

software inspections : analysis of the static system representation of discover problems maybe tool based
## Maybe this called as static verification
## static verification -> bad patterns exmp nullability, code conventions
needs people examples -> requirements design, configuration data, test data

advantages: 
1. errors can mask other errors so don't need to be concerned with interactions betweeen 
errors
2. incomplete parts of a system can be inspected without additional cost
## 3. WTF? As well as searching for program defects, an inspection can also consider broader quality attributes 
of a program, such as compliance with standards, portability and maintainability. 

#Stages of testing

1. development testing -> where the system is tested during development to discover bugs and defects
2. release testing -> where a seperate testing team complete version testing before release
3. user testing -> potential users tests


#Development testing:

unit testing -> 
individual program units or object classes are tested
- individual component tested in isolation
- defect testing process

these are under unit testing header

object class testing
- complete test coverage of a class involves
- inheritance makes harder

automated testing
- unit test should be automated


testing strategies
1. partition testing: 
- inputs have common characteristics and should be processed in the same
way
- choose tests from each of these groups
- each of the tested classes is an equivalance partition or domain where the program
behaves in a equivalent way for each class member
- test cases should be chosen from each partition
2. guideline-based testing
- experience of developers?
- use testing guidelines
example: general ones
1. choose inputs that force system to errors
2. repeat the same input or series of inputs numerous times
3. force invalud outputs to be generated
4. force computation results to be too large or too small

example: testing guidelines sequences
1. test with sequences which have only a single value
2. use sequences different sizes
3. test with zero length
4. first middle last element sequences

## FINISH unit testing 

component testing -> individual units composed and focused on interfaces
- composite objects accesed via interfaces so test that?
# think that as a module testing

interface testing
firstly interface types -> 
parameter interfaces : data passed one method to another one
shared memory interfaces: block memory shared between procdeures or functions
procedural interfaces -> subsystem one

interface errors
1. interface misuse -> ex parameters inorder, wrong interface call
2. interface misunderstanding -> dev assumptions are wrong which are incorret calls
3. timing errors -> operation operate different speeds ex-> async await

interface testing:
1. design tests so the paramters to a called procdeure at the extreme ends of
their ranges
2. always test pointer parameters with null pointers
3. design tests which cause the componet to fail
4. stress test use at message passing systems
5. shared memory try to call different order
## stress testing -> load test-alike extreme traffic example
## example one service uses queue underneath -> try to overflow it so this testing becomes hard

system testing -> system tested as a whole focused on component interactions
## don't care the undrlying code, evulates the behavior of the program, after code complete



regression testing -> testing the system to check that changes have not broken previously
working code

release testing -> black-box testing system // system-specificication --> it is a system testing
also a validation testing


requirement based test -> examine requiremnt and write test for that
example -> mhc-pms requirements :
if a patient has a known allergic reaction mediciation should be warned if potential alergic presprection
if warning message ignored, should be input for that implies why


performance testing->  maybe part of release testing
stress test is a form of performance testing


user testing  -> customer testing essential


types of user testing
alpha: developers and users work together
beta: made available to the users with system developers
acceptance: customers test product is ready or not



program inspections
- formalised approach to document reviews
- intended explicitly for defect detection not correction
- defects may be logical, erronneous condition uninitialized variable

inspection have pre-conditions
- precise specificication must be available
- team members knows organizaiton standards
- sytnax should be correct and system reporesentations must be available
- error checklist
- management accept that inspections increase costs in early software process
- management shouldn't use inspections for who is making mistake


inspections roles
author -> programmer designer  responsible for producing the program
inspector ->  error finder
reader -> inspection meeting reports code?
scribe -> recorder of the meeting
chairman -> report forwarder
chief moderator -> responsible for action taking

// @TODO
inspections checklist
- wtf ? 


automated static analysis -> potential software source code checkers that find potential errors
ex -> sonarlint, 

stages of static analysis

control flow analysis -> if else unreachable code,
data use analysis -> detects uninitialized, used constants variables,
interface analysis -> consistency of routien and procedure
information flow analysis -> does not detect anomalies itself, dependencies of output
variables
path analysis -> identifies paths through the program and sets out the statements (example repository like)